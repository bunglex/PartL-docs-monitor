name: "PartL Monitor â€” Crawl & Publish"

on:
  push:
    branches: [ master ]
  workflow_dispatch: {}
  schedule:
    - cron: "17 7 * * 1"   # Mondays 07:17 London

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  build-and-publish:
    runs-on: ubuntu-latest
    env:
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Prepare output folders
        run: |
          mkdir -p docs docs/archive docs/diffs history texts archive diffs registry

      - name: Crawl (write registry/current.json)
        run: |
          python scripts/check_partl.py --rules rules --out registry/current.json --debug

      - name: Render static site (to docs/)
        run: |
          python scripts/render_site.py --registry registry/current.json --templates templates --out docs

      - name: Configure Pages
        uses: actions/configure-pages@v5

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: docs

      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4
